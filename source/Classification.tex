\section*{Classification}
\subsection*{0/1 loss - "NP-Hard"}
0/1 loss is not convex and not differentiable.\\
$l_{0/1} (w;y_i,x_i) =
\begin{cases}
    1 \text{ , if } y_i \neq sign(w^Tx_i)\\
		0 \text{ , otherwise} 
\end{cases}$

\subsection*{Perceptron loss}
Perceptron loss is convex and not differentiable, but gradient is informative.\\
$l_{P} (w;y_i,x_i) = max\{0, -y_i w^T x_i \}$\\
$\nabla_w l_p(w;y_i,x_i) = \begin{cases}
    0 &\text{ , if } -y_i w^T x_i \leq 0\\
    -y_i x_i &\text{ , if } -y_i w^T x_i > 0
\end{cases}\\
w^* = \underset{w}{\operatorname{argmin}} \sum_{i=1}^n l_p (w;y_i,x_i)$

\subsection*{Stochastic Gradient Descent (SGD)}
1. Start at an arbitrary $w_0 \in \mathbb{R}^d$\\
2. For $t = 1, 2,  ...$ do: \\
	Pick data point $(x',y') \in_{u.a.r.} D$\\
	$w_{t+1} = w_t - \eta_t \nabla_w l(w_t;x',y')$\\
Perceptron Algo: SGD with Perceptron loss

%\subsection*{Perceptron Algorithm}
%Stoch. Gradient Descent with Perceptron loss\\
%\emph{Theorem:} If $D$ is linearly separable $\Rightarrow$ Perceptron will obtain a linear separator.

%\subsection*{Hinge loss}
%Loss for Support Vector Machine.\\
%$l_H(w;x,y) = max \{0,1-y w^T x\}$

\subsection*{Support Vector Machine - "Max Margin"}
Hinge loss: $l_H(w;x,y) = max \{0,1-y w^T x\}$\\
Goal: max. the margin around the separator.\\
$w^* = \underset{w}{\operatorname{argmin}} \sum_{i=1}^n  max \{0,1-y_i w^T x_i\} + \lambda ||w||_2^2\\
g_i(w) = max \{0,1-y_i w^T x_i\} + \lambda ||w||_2^2\\
\nabla_w g_i(w) = \begin{cases}
    -y_i x_i + 2\lambda w &\text{ , if $y_i w^T x_i<1$}\\
		2\lambda w &\text{ , if $y_i w^T x_i \geq 1$}
\end{cases}$

\subsection*{L1-SVM}
$\underset{w}{\operatorname{min}} \lambda ||w||_1 + \sum_{i=1}^n max(0,1-y_i w^T x_i)$ 
$\rightarrow$ enourages coefficients to be zero (only linear models).

%\subsection*{Matrix-Vector Gradient}
%multiply transposed matrix to the same side as its occurance w.r.t. derivate variable: $\beta \in \mathbb{R}^d$
%$\nabla_\beta ( ||y-X\beta||_2^2 + \lambda ||\beta||_2^2 ) = 2X^T (y-X\beta) + 2\lambda \beta$\\
