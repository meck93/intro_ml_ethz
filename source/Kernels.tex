\section*{Kernels = Non-Parametric}
\subsection*{Reformulating the perceptron}
Ansatz: $w=\sum_{j=1}^n \alpha_j y_j x_j$\\
$\min \limits_{w\in\mathbb{R}^d} \sum_{i=1}^n \max [0, -y_i w^T x_i]$\\
%$= \min \limits_{\alpha_{1:n}} \sum_{i=1}^n \max [0,-y_i  ( \sum_{j=1}^n \alpha_j y_j x_j  )^T x_i ]$\\
$= \min \limits_{\alpha_{1:n}} \sum_{i=1}^n \max  [0,- \sum_{j=1}^n \alpha_j y_i y_j x_i^T x_j ]$

%\subsection*{Polynomial kernel}
%$k(x,y) = (x^Ty)^m$  all monomials of deg. m \\
%$k(x,y) = (1+x^Ty)^m$ all monomials up to deg. m

\subsection*{Kernelized Perceptron}
1. Initialize $\alpha_1 = ... = \alpha_n = 0$\\
2. For $t = 1, 2, ...$ do \\
Pick data $(x_i,y_i) \in_{u.a.r} D$\\
Predict $\hat{y} = sign(\sum_{j=1}^n \alpha_j y_j k(x_j,x_i))$\\
If $\hat{y} \not = y_i$ set $\alpha_i = \alpha_i + \eta_t$\\
Predict new point x: $\hat{y} = sign(\sum_{j=1}^n \alpha_j y_j k(x_j,x))$

\subsection*{Properties of a Kernel}
k must be a function: $f: X \times X \rightarrow R$\\
k must be symmetric: $k(x,y) = k(y,x)$\\
Matrix K must be positive semi-definite (psd).

\subsection*{Kernel Matrix K}
$K = 
\begin{bmatrix}
	k(x_1,x_1) & \dots & k(x_1,x_n) \\
	\vdots & \ddots & \vdots \\
	k(x_n, x_1) & \dots & k(x_n,x_n)
\end{bmatrix}$\\
positive semi-definite matrices $\Leftrightarrow$ kernels
%$\left ( XX^T \right )$ for inner product as kernel.

\subsection*{Definition of PSD}
$M \in \mathbb{R}^{n\times n}$ is psd $\Leftrightarrow$\\
$\forall x \in \mathbb{R}^n: x^TMx \geq 0 \Leftrightarrow$\\
all eigenvalues of $M$ are positive: $\lambda_i\geq 0$

\subsection*{Nearest Neighbor k-NN}
$y=sign(\sum_{i=1}^n y_i [x_i \text{ among k nn of } x])$

\subsection*{Examples of kernels on $\mathbb{R}^d$}
Linear kernel: $k(x,y)=x^T y$\\
Polynomial kernel: $k(x,y)=(x^T y + 1)^d$\\
Gaussian kernel: $k(x,y) = exp(-||x-y||_2^2/h^2)$\\
Laplacian kernel: $k(x,y) = exp(-||x-y||_1/h)$\\
h = bandwidth $\approx 1\sigma$ 


\subsection*{Kernel Properties / Rules}
$k_1(x,y) + k_2(x,y)$; $k_1(x,y) \cdot k_2(x,y)$; $c \cdot k_1(x,y)$,$c>0$;\\
$f(k_1(x,y))$, where $f$ is a polyinomial with pos. coeffs. or the exponential function

%without Parametric vs. Nonparametric
\iffalse
\subsection*{Parametric vs. Nonparametric}
\emph{Parametric}: have finite set of parameters\\
E.g. linear regression, linear perceptron,...\\
%$f(x) = w^Tx, w\in \mathbb{R}^d$ (d is independent of \# data)\\
\emph{Nonparametric}: grow in complexity with the size of the data\\
E.g. kernelized Perceptron, k-NN,...
%$f(x) = \sum_{i=1}^n \alpha_i y_i k(x_i,x_n)$ (depends on \# data)\\
\fi

\subsection*{Perceptron and SVM}
Perceptron: $\underset{\alpha}{min}\sum_{i=1}^n max\{0,-y_i \alpha^T k_i\}$\\
SVM: $k_i=[y_1 k(x_i,x_1), ..., y_n k(x_i,x_n)]$:\\
$\underset{\alpha}{min}\sum_{i=1}^n max\{0,1-y_i \alpha^T k_i\} +\lambda\alpha^T D_y K D_y \alpha$\\
Prediction: $y = sign(\sum_{j=1}^n \alpha_j y_j k(x_j,x))$

\subsection*{Kernelized linear regression}
Ansatz: $w^*=\sum_i \alpha_i x$\\
Parametric: $w^* = \underset{w}{\operatorname{argmin}} \sum_i (w^Tx_i-y_i)^2 + \lambda ||w||_2^2$\\
%$= \underset{\alpha_{1:n}}{\operatorname{argmin}} \sum \limits_{i=1}^n (\sum \limits_{j=1}^n \alpha_j x_j^T x_i - y_i)^2 + \lambda \sum \limits_i \sum \limits_j \alpha_i \alpha_j (x_i^T x_j)$\\
%$= \underset{\alpha_{1:n}}{\operatorname{argmin}} \sum \limits_{i=1}^n (\alpha^T K_i - y_i)^2 + \lambda \alpha^T K \alpha$\\
$= \underset{\alpha}{\operatorname{argmin}} ||\alpha^T K -y||_2^2 + \lambda \alpha^T K \alpha$\\
Closed form: $\alpha^* = (K+\lambda I)^{-1} y$\\
Prediction: $y = w^{*T} x = \sum \limits_{i=1}^n \alpha_i^* k(x_i,x)$