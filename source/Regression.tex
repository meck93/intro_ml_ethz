\section*{Regression}
\subsection*{Linear Regression}
Error: $\hat{R}(w) = \sum_{i=1}^n (y_i - w^Tx_i)^2 = ||Xw-y||^2_2$\\
$w^* = \underset{w}{\operatorname{argmin}} \sum_{i=1}^n (y_i - w^Tx_i)^2$\\
Closed form: $w^*=(X^T X)^{-1} X^T y$\\
$\nabla_w \hat{R}(w) = -2 \sum_{i=1}^n (y_i-w^T x_i) \cdot x_i = 2X^T (Xw-y)$

\subsection*{Convex / Jensen's inequality}
$\text{g(x) is convex} \Leftrightarrow x_1,x_2 \in \mathbb{R}, \lambda \in [0,1]: g''(x) > 0$\\
$g(\lambda x_1 + (1-\lambda) x_2) \leq \lambda g(x_1) + (1-\lambda) g(x_2)$

\subsection*{Gradient Descent}
1. Start arbitrary $w_o \in \mathbb{R}$\\
2. For $t = 1,2,...$ do $w_{t+1} = w_t - \eta_t \nabla \hat{R}(w_t)$

\subsection*{Expected Error (True Risk)}
Assumption: data set generated iid: $R(w) =$\\ 
$\int P(x,y) (y-w^Tx)^2 \partial x \partial y = \mathbb{E}_{x,y}[(y-w^Tx)^2]$\\
$\hat{R}_D(w) = \frac{1}{|D|}\sum_{(x,y)\in D} {(y-w^Tx)^2}$ (estim. error)

\subsection*{Gaussian/Normal Distribution}
$\sigma =$ standard deviation, $\sigma^2 =$ var., $\mu =$ mean:\\
$f(x) = \frac{1}{\sqrt{2\pi\sigma^2}} exp(-\frac{(x-\mu)^2}{2\sigma^2})$

\subsection*{L2-reg: Ridge Regression}
Regularization: $\underset{w}{\operatorname{min}} \sum \limits_{i=1}^n (y_i - w^Tx_i)^2 + \lambda ||w||_2^2$\\
Closed form solution: $w^*=(X^T X + \lambda I)^{-1} X^T y$\\
$(X^T X + \lambda I)$ always invertible.\\
Gradient: $\nabla_w \hat{R}(w) = -2 \sum \limits_{i=1}^n (y_i-w^T x_i) \cdot x_i + 2 \lambda w$

%\subsection*{L1-regularized regression (the Lasso)}
%Regularization: $\underset{w}{\operatorname{min}} \sum \limits_{i=1}^n (y_i - w^Tx_i)^2 + \lambda ||w||_1$\\
%Encourages coefficients to be exactly 0.

\subsection*{Standardization}
Goal: each feature: $\mu = 0$, unit $\sigma^2$: $\tilde{x}_{i,j} = \frac{(x_{i,j}-\hat{\mu}_j)}{\hat{\sigma}_j}$\\
$\hat{\mu}_j = \frac{1}{n}\sum_{i=1}^n x_{i,j}$, $\hat{\sigma}_j^2 = \frac{1}{n}\sum_{i=1}^n {(x_{i,j}-\hat{\mu}_j)}^2$ 


%\subsection*{Multivariate Gaussian}
%$\sigma =$ covariance matrix, $\mu$ = mean\\
%$f(x) = \frac{1}{2\pi \sqrt{|\Sigma|}} e^{- \frac{1}{2} (x-\mu)^T \Sigma^{-1} (x-\mu)}$

%\subsection*{Regularization}
%The error term $L$ and the regularization $C$ with regularization parameter $\lambda$: $\min \limits_w L(w) + \lambda C(w)$\\
%L1-regularization for number of features \\
%L2-regularization for the length of $w$

%my idea
%\subsection*{Regularization}
%A lot of supervised learning problems can be written in this way: $\lambda$: $\min \limits_w \hat{R}(w) + \lambda C(w)$\\